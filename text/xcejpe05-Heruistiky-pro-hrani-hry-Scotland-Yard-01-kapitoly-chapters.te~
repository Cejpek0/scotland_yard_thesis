\newtheorem{definition}{\textbf{Definice}}

\chapter{Úvod}
\label{ch:uvod}

V~dnešní době, je obor umělé inteligence (AI), postupem let všudypřítomnější a její vliv ovlivňuje spoustu aspektů našeho života.

Pokrok umělé inteligence je často a nejlépe měřen její aplikováním v~oblasti her.
A~to proto, že právě hry nabízí jasně definovaná pravidla a cíle, čímž se stávají ideálním testovacím prostorem pro algoritmy umělé inteligence.
Výkon AI je ve hrách snadno měřitelný a pokrok dokáže vidět i lajk bez žádných složitých grafů a výpočtů.
Umělá inteligence již dokázala porazit nejlepší hráče v~\textit{šachu}~\cite{DeepBlue}, \textit{Dota 2}~\cite{Dota2}, \textit{Go}~\cite{AlphaGo} a spoustě dalších her.

Hra studovaná v~této práci se jmenuje \emph{Scotland Yard}.
Jedná se o~hru pro tři až šest hráčů.
V~této hře obvykle hraje jeden hráč jako Pan~X, který se snaží uniknout policistům, ovládanými ostatními hráči.
Policisté, avšak nevědí, kde na herním poli se Pan~X nachází.
A~musí tedy odhadovat jeho pozici a spolupracovat mezi sebou, aby ho mohli polapit.
Pozice Pana~X je policistům odhalena pouze v~určitých kolech.
Hra je u~konce, když Pan~X chycen policisty (vyhrávají policisté), nebo když je dosažen maximální počet kol (vyhrává Pan~X).
Scotland Yard je ideální hrou ke studování neurčitosti ve hrách hranými systémy umělé inteligence, protože se jedná o~hru s~nedokonalou informací a k~vítězství policistů je zapotřebí spolupráce a strategie.

Tato práce zkoumá algoritmy posilovaného učení, jejich použití na hry s~neurčitostí a jejich porovnání s~klasickými metodami hraní her.
Algoritmus proximální optimalizace strategie (Proximal Policy Optimization--PPO) je hlavním zkoumaným algoritmem této práce.
Pro srovnání s~jinými algoritmy slouží algoritmus Deep--Q--Learning (DQN).
PPO je často používán k~řešení problémů se spojitými veličinami a ve 3D prostoru.
Algoritmus PPO byl vybrán z~důvodu jeho úspěšnosti při použití ve složitých hrách s~neurčitostí.
Tyto tvrzení podporuje experiment OpenAI~\cite{PPO_Hide_Seek_page}, podrobně popsán zde~\cite{PPO_Hide_Seek_paper}.
V~tomto experimentu byla proximální optimalizace strategie použita pro učení strategie ve hře typu \textit{schovávaná} s~několika agenty a neúplnou informací.
Jelikož tento typ hry přesně odpovídá i hře Scotland Yard, zdá se být vhodný i pro řešení této hry.
Také další studie~\cite{Manille} a~\cite{Dota2} tento výrok dále podporují a dokazují vhodnost algoritmu PPO pro tuto práci.

Zaměření této práce jsem si vybral, jelikož mi vždy byla umělá inteligence blízká a vždy jsem chtěl začít tomuto odvětví více věnovat i po praktické stránce.
Algoritmus je pro mě zajímavý a zkušenost s~tímto algoritmem by se dala využít v~mém dalším pracovním životě.\newpage

Pro zpracování práce byly využity tyto hlavní knihovny:
\begin{itemize}
  \item \texttt{Ray.Rlib} - framework pro posilované učení. Obsahuje již zhotovenou implementací algoritmu PPO a DQN, které byly použity v~této práci. 
  \item \texttt{PyTorch} - podpůrná knihovna \texttt{Ray.Rlib}
  \item \texttt{Gymnasium} - knihovna použita k~vytvoření prostředí hry Scotland Yard
  \item \texttt{Pygame} - knihovna pro vytváření uživatelského rozhraní
\end{itemize}

Práce byla zhotovena pro operační systémy Windows a Python verze 3.10

\chapter{Shrnutí dosavadního stavu}
\label{ch:dosavadni_stav}
Tato kapitola není encyklopedickým přehledem celého tématu bakalářské práce.
Nýbrž jedná se o~shrnutí nejdůležitějších relevantních informací a pojmů, důležitých pro tuto práci.

\section{Desková hra Scotland Yard}
\label{sec:deskova-hra-scotland-yard}

Scotland Yard je populární hra pro tři a více hráčů, která kombinuje prvky schovávané a hry na honěnou.
Jeden hráč hraje za Pana~X, který se snaží uniknout policistům a ty ovládají zbylí hráči.
Hra končí, když je Pan~X chycen (vyhrávají policisté), nebo když je dosažen maximální počet kol (vyhrává Pan~X).
Originální hra se odehrává v~na mapě Londýna.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{obrazky-figures/scotland_original}
	\caption{Ukázka herní mapy hry Scotland Yard.
  Zdroj~\cite{scotland_original_image}:}\label{fig:figure}
\end{figure}
\newpage

Na této herní mapě nachází 200 polí, které jsou vzájemně propojené náhodnými cestami.
Každá z~těchto cest povoluje pouze určitý způsob dopravy (např. pouze taxíkem, pouze autobusem atd.).
Jednotliví hráči využívají prvky veřejné dopravy k~pohybu po herní ploše, kterými jsou:

\begin{itemize}
  \item \emph{Taxi}\vspace{-0.3cm}
  \item \emph{Autobus}\vspace{-0.3cm}
  \item \emph{Metro}\vspace{-0.3cm}
  \item \emph{Trajekt}\vspace{-0.3cm}
\end{itemize}


Každému hráči je na začátku hry přidělen pouze určitý počet jízdenek na tyto dopravní prostředky.
K~využití dopravy je potřebná právě tato jízdenka.
Pokud jízdenku hráč nemá, nemůže již tento způsob přepravy použít.
Hra se dělí na kola, ve kterých se hráči střídají.

Hlavní zápletkou hry je, že po většinu kol je pozice Pana~X policistům utajena.
Odhaluje se jim pouze určená kola.
To znamená, že policisté pro polapení Pana~X musí odhadovat jeho pozici a spolupracovat mezi sebou.
Tímto se ze hry Scotland Yard stává hra s~nedokonalou informací, jelikož policisté nevidí přesnou pozici Pana~X\@.
Tento fakt ji činí vhodnou pro studování metod hraní her s~neurčitostí.

\section{Další hry s~nedokonalou informací}\label{sec:dalsi-hry-s-nedokonalou-informaci}
V~oblasti umělé inteligence hraje důležitou roli modelování a řešení her.
Hry představují formalizaci konfliktních interakcí mezi aktéry.
Klasická teorie her se zaměřuje na hry s~úplnou informací, kde mají všechny strany v~daném okamžiku přístup ke všem relevantním informacím z~herního prostředí.
Znají strategie a cíle ostatních hráčů.
V~praxi se však častěji setkáváme se situacemi, kde jednotlivým stranám chybí některé informace, ať už se jedná o~prostředí či cíli soupeře.

\subsection{Bayesovské hry}
Tyto případy lze modelovat pomocí her s~neúplnou či nedokonalou informací, kde hráči nemají úplné znalosti o~prostředí nebo soupeřích.
Neúplnou informaci můžeme sledovat například u~Bayesovských her.

\begin{definition}[Bayesovská hra]
\cite{Y_Narahari} je definována pěticí $(N, A_i, \theta_i, p(\theta_i), u_i)$, kde:

\begin{itemize}
\item $N$ je konečná množina hráčů, $N = \{1, 2, \ldots, n\}$.
\item $A_i$ je neprázdná množina strategií hráče $i$.
\item $\theta_i$ je neprázdná množina typů hráče $i$.
\item $p(\theta_i)$ je apriorní pravděpodobnostní rozdělení typu hráče $i$ na $\theta_i$.
\item $u_i: A_1 \times \cdots \times A_n \times \theta_1 \times \cdots \times \theta_n \rightarrow \mathbb{R}$ je výplatní funkce hráče $i$.
\end{itemize}
\end{definition}

Bayesovské hry představují formální rámec sloužící k~modelování a popisu her s~neúplnou informací.

\subsection{Stratego}\label{subsec:stratego}
Stratego je desková strategická hra pro dva hráče.
Vychází z~dřívějších her, jako je Šachy a Go, a kombinuje strategické plánování se snahou obelstít soupeře.
Hra se odehrává na herní ploše o~velikosti 10x10 polí.
Na herním poli jsou umístněny sekce s~různými terény, které znemožňují pohyb jednotek.
To vytváří různé strategicky uzká místa, kde je možno zabarikádovat protivníka jen pomocí dvou figur.
Tím se hra stává zajímavější a je možné tyto body využívat k donucení oponenta aby zaútočil jako první.

Na této herní mapě má každý hráč svoji sadu figur reprezentujících armádu.
Figury jsou ale vyobrazeny tak, že jejich hodnost lze přečíst pouze z~jedné strany.
Tím druhý hráč nezná, rozestavění protivníkovy armády.
Každý hráč má 40 figur, rozdělených do 11 hodností (generál, plukovník, skaut atd.).

Cílem hry je porazit soupeře nalezením a obsazením jeho vlajky.
S~vlajkou, stejně tak s~minou nemohou hráči po umístnění pohnout.
Tyto speciální miny mohou zničit jakoukoliv figuru se kterou se utkají v~boji.
Minu může zničit pouze horník.

Hra začíná tím, že každý hráč rozmístí své figury na herní pole.
Hráči se střídají v~tazích, kdy se pokouší najít oponentovu vlajku.
Pokud hráč táhne na pole, kde se nachází oponentova figura, nastává souboj.
Souboj spočívá v~odkrytí hodnosti obou figur a vyhrává ta s~vyšší hodností.
Figura, která vyhrála zůstává, poražená figura je odstraněna z~hry.

Ve hře Stratego je důležité blafování a odhadování soupeřových tahů.
V~této hře velmi důležitá informace, tudíž je důležité informace získávat.
To můžou hráči učinit například obětováním slabších figur, aby odhalily silnější hodnosti.
I tato hra je modelovatelná metodou Bayesovských her \cite{Bayes_stratego}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{obrazky-figures/stratego}
	\caption{Ukázka rozestavěných figur ve hře Stratego.
  Zdroj~\cite{Stratego_image}:}\label{fig:figure2}
\end{figure}

Z~pohledu umělé inteligence je Stratego zajímavý problém.
Nejenže je hra s~nedokonalou informací a je tedy zapotřebí odhadovat oponentovy tahy a blafovat.
Hra ale také má obrovský stavový prostor $10^{535}$~\cite{Perolat_2022}, čímž je prakticky neřešitelná klasickými metodami, které prochází celý stavový prostor.
Až do roku 2022 nebyla umělá inteligence v~této hře velmi úspěšná a nedokázala se rovnat expertním hráčům.
To se změnilo příchodem \emph{DeepNash}~\cite{Perolat_2022}, kdy se tento agent umístil mezi třemi nejlepšími hráči světa.

\subsection{Dota 2}\label{subsec:dota}
Dota 2 je velmi komplexní strategická hra v~odehrávající se v~reálném čase.
Hraje se ve dvou týmech po pěti hráčích.
Každý tým má svoji základnu, kterou se snaží chránit a zároveň zničit základnu soupeře.
Hra je u~konce v~okamžiku, kdy je zničena základna jednoho z~týmů a tento tým prohrává.
Herní mapa se skládá ze tří hlavních cest, které jsou spolu navzájem propojeny mnoha menšími stezkami.

V~každém týmu hraje 5 hráčů, kteří ovládají postavy s~různými unikátními schopnostmi.
Hráči získávají zkušenosti jak za porážení nepřátelských postav, tak za zabití jednotek ovládaných počítačem.
Zkušenosti se využívají k~zvyšování úrovně postavy a odemykání nových schopností.
Schopnosti jsou hlavní součástí hry.
Díky schopnostem mohou hrdinové zranit nepřítele, udělat silnější sebe či své spojence a mohou mít i spousty dalších efektů.
Co jim navíc přidává na komplexnosti je to, že každá schopnost nemůže být po použití znovu použita po určitou dobu.
Zlato používají hráči jako herní měnu, za kterou si hrdinové mohou koupit různé předměty, které je posílí.

Ve hře se vyskytuje válečná mlha, která způsobuje že hráči vidí pouze část mapy ve svém okolí a okolí spřátelených jednotek.
To znamená, že hráči nemají úplnou informaci o~pozici nepřátelských jednotek.

\bigskip

Dota 2 je pro umělou inteligenci náročnou výzvou z~těchto důvodů:
\begin{itemize}
  \item Složitost hry
  Dota 2 je komplexní hra s~mnoha proměnnými a možnostmi.
  Její stav se mění velmi rychle a je tedy těžké předpovědět vývoj hry.
  Agenti tedy musí být schopni rychle reagovat na změny a tím přizpůsobit svoji strategii.
  \item Nedokonalá informace
  Právě kvůli válečné mlze nemají hráči úplnou informaci o~pozici nepřátelských jednotek a musí tedy pouze odhadovat kde na mapě se nachází.
  Neznají ani jestli jsou nepřátelé schopni použít své schopnosti a předměty.
\end{itemize}

Dota 2 sama o~sobě obsahuje implementaci chování umělé inteligence.
Její výkon však není dostatečný a nebyla schopna ani zdaleka porazit nejlepší hráče světa.
Použité metody pro vytvoření těchto agentů bohužel nejsou zveřejněny.

\ref{subsec:dota}
Inovaci umělé inteligence v~této hře opět přinesla společnost OpenAI a jejich projekt \emph{OpenAI Five}~\cite{Dota2}.
Tento projekt se zaměřil na vytvoření agenta, který by byl schopna porazit nejlepší hráče světa ve hře Dota 2.
OpenAi nepřišel se žádným novým algoritmem, ale pouze s~novým přístupem k~trénování a využití algoritmu \textbf{PPO} v~doposud neviděném měřítku. 
Algoritmus PPO a posilované učení bodou blíže popsáni v~následujících sekcích~\ref{sec:proximalni-optimalizace-strategie} a~\ref{sec:klicove-koncepty-posilovaneho-uceni}.

OpenAi Five porazil tehdejší nejlepší tým světa během exhibice na nejprestižnějším turnaji Dota2 \textit{The International}.
Hrála se klasická turnajová verze, a to tedy nejlepší ze tří.
OpenAI Five byl schopen porazit nejlepší tým světa s~rozhodným výsledkem 2:0.
První hra trvala 38~minut a druhá již pouhých 21~minut.
V~celém exhibičním turnaji proti nejlepším týmům světa z~celkových 24 her vyhrál 19 her a pouhých 5 prohrál.

Agent OpenAI Five byl, kombinovaně s~předchozí verzí, trénován asi \emph{55 000} herních let hraním sama proti sobě.
Pozorovací prostor je obrovský, obsahuje až 16 000 vstupů.
Akční prostor je složitý, jelikož lidé hrají Dotu 2 většinou pomocí myši a klávesnice.
U~OpenAI Five je akční prostor rozdělen na hlavní akce a parametry těchto akcí.
Hlavní akce jsou například, pohyb, útok, použití schopnosti/ předmětu, zakoupení předmětu a další situační akce.
Dostupnost těchto akcí je závislá na stavu hry a je řízena maskou akcí.
Tyto akce mají 3 parametry, zpoždění, vybranou jednotku a offset cíle akce.
Pokud tyto akce a jejich parametry zkombinujeme získáme akční prostor o~velikosti 1,837,080 dimenzí.
Výsledný reakční čas agenta je okolo 33ms, oproti tomu reakční čas profesionálních hráčů her je okolo 120ms~\cite{reaction_time}

Na začátku byla většina chování agenta uměle naprogramované a postupně mu byla předávána kontrola nad vlastním chováním s~využití strategie.
Některé naskriptované chování však bylo zachováno i ve finální verzi OpenAi Five a nebyly odstraněny, jelikož již agent byl dostatečně efektivní a nebylo třeba je odstranit.

\section{Klíčové koncepty posilovaného učení}\label{sec:klicove-koncepty-posilovaneho-uceni}
Posilované učení (Reinforcement Learning, RL) je oblast strojového učení, která se zaměřuje na učení agentů v~dynamickém prostředí.
Agent se učí strategii chování, která maximalizuje kumulativní odměnu.

\subsection{Agent}\label{subsec:agent}

Je komplexní entita, která interaguje s~prostředím.
Prostředí poskytuje agentovi informace o~stavu a agent na základě těchto pozorování vykonává akce.
Tyto akce mohou ovlivnit stav prostředí a agent obdrží odměnu na základě odměnové funkce.
Agent většinou volí takové akce, aby maximalizoval kumulativní odměnu.

\subsection{Pozorovací a akční prostor}
\label{subsec:prostory}

\textbf{Pozorovací prostor} je jednou z~nejdůležitějších součástí posilovaného učení.
Bez jeho vhodného zvolení, je dosažení strategie s~dobrými výsledky velmi obtížné.
Pozorovací prostor je množina všech možných pozorování, které může agent získat z~prostředí.
Je to tedy forma, kterou prostředí předává informace agentovi.
Velmi často se aktuální stav pozorovacího prostoru využívá jako stav.
Při vytváření prostředí je nutné definovat typ, tvar prostoru a jaké hodnoty může nabývat.

Pro jeho podoby je vhodné zvolit standard z~knihovny \texttt{Gymnasium}~\cite{Gymnasium}.
Gymnasium je pokračování knihovny \texttt{Gym} od společnosti OpenAI\@.
Tyto knihovny nastavily standard vytváření prostředí pro posilované učení.
Dodržování těchto standardů umožňuje použití libovolného algoritmu na jedno prostředí.

\bigskip

\textbf{Akční prostor} je, jak název vypovídá, množina všech možných akcí, ze kterých může strategie volit.
Často se jedná o~množinu celočíselných hodnot, které reprezentují různé akce.
Samozřejmě může být akční prostor i spojitého typu (blíže popsáno v~další sekci).

V~herním prostředí mohou nastat situace kdy se nějaká akce stane nevalidní, jelikož by její provedení vyústilo v~nevalidní stav herního prostředí.
V~takovém případě je vhodné zvolení této akce zamezit, nejlépe pomocí akční masky.
Ta definuje, které akce jsou v~momentálním stavu prostředí proveditelné.

\subsection{Diskrétní a spojitý prostor}\label{subsec:diskretni-a-spojity-prostor}

Pokud se bavíme o~\textbf{diskrétním prostoru} v~oboru umělé inteligence, bavíme se o~konečném prostoru. 
Skládá se tedy z~konečného počtu prvků.
Příkladem takového prostoru je například pozorovací prostor, jehož vstupem je fotka.
Ta se dělí na přesně daný počet pixelů a je tedy diskrétní.
Diskrétní akční prostory jsou počítatelné a často se jedná o~konečnou množinu možných akcí.

Zato \textbf{spojitý prostor} je nekonečný prostor, který může obsahovat nekonečný počet prvků. 
Spojitý pozorovací prostor je například ten, jehož vstupem je audio stopa.
Spojité akční prostory jsou nekonečné a často je definuje nekonečná množina akcí.
Na prostředí se spojitými prostory se často používají jiné algoritmy, například algoritmus DDPG (Hluboký deterministický gradient strategie).

\bigskip

Uvažujme však 3D hru kde se agent může hýbat za pomocí aplikování silového vektoru o~různé velikosti a v~různém směru.
Může tím tedy měnit rychlost a směr pohybu.
V~tomto případě se jedná o~\textbf{spojitý akční prostor}.

\subsection{Prostředí}\label{subsec:prostredi2}

Je vše s~čím agent interaguje.
Prostředí je buď fyzické (entity z~reálného světa, ovládání chytré domácnosti, robotické ruky, ovládání reaktoru apod.) nebo virtuální (simulace nebo hra).
Prostředí reaguje na zvolené akce agenta poskytuje mu zpětnou vazbu ve formě odměny.
Odměna může být i záporná, pokud agent zvolil velmi nevhodnou akci.
Pokud v~prostředí existuje více agentů, může mít každý agent jiné pozorování.
Díky tomuto můžeme například schovat agentovi \textit{A} určité informace, které agent \textit{B} vidí.

\subsection{Strategie (Policy)}\label{subsec:strategie}

Pomocí posilovaného učení vzniká strategie.
Strategie je matematická funkce, která definuje agentovo chování na základě jeho pozorování (stavu).
Snaží se definovat takové chování, které vede k~maximální kumulativní odměně.
Strategie může být deterministická nebo stochastická.

  \subsubsection*{Deterministická strategie}
  
  Deterministická strategie přesně definuje cílový stav přechodu pro každý stav.
  Agent tedy pro jeden stav vždy volí stejnou akci.
  Tato strategie je vhodná, pokud je zapotřebí v~každém stavu reagovat konzistentně, bez odchylek.
  Například, pokud agent ovládá termostat v~domě a teplota je pod požadovanou hladinu.
  Nemůže se stát, aby byla šance, že agent zvolí akci, která teplotu ještě sníží.
  Další výhoda, je že je jednoduchá na interpretaci a implementaci~\cite{Policies}.

  Rovnice výsledné akce deterministické strategie je:
  \begin{equation}
    \pi(s) = a\label{eq:policy_deterministic}
  \end{equation}

  \subsubsection*{Stochastická strategie}
  
  Zato stochastická strategie definuje pro každý stav pravděpodobnostní rozdělení nad množinou akcí.
  Výsledná akce je tedy náhodná dle rozdělení pravděpodobnosti.
  Může tedy nastat situace kdy ve stejném stavu agent zvolí vždy jinou akci.
  Tato strategie je vhodná v~situacích, kdy je potřeba zkoumat různé strategie a kdy agent nemá úplnou informaci o~prostředí.
  Například tam kde by deterministická strategie zvolila jasnou akci \textit{A}, stochastická strategie by mohla s~malou pravděpodobností zvolit akci \textit{B}.
  Čímž ale může odhalit, že stav \textit{B} je s~ohledem na komulativní odměnu lepší než stav \textit{A}~\cite{Policies}.

  Rovnice výsledné akce stochastické strategie je:
  \begin{equation}
    \pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]\label{eq:policy_stochastic}
  \end{equation}

\subsection{Akce}\label{subsec:akce}

Akce je aktivita, proces či funkce kterou agent vykoná ve specifickém stavu~\cite{ActionCo67}.
Výsledkem provedení akce je tedy změna z~aktuálního stavu, do jiného, či stejného stavu z~množiny možných stavů.
Zjednodušeně, je to rozhodnutí, které agent vykonává v~prostředí a toto rozhodnutí ovlivňuje prostředí.

Akce může být také nedeterministická či stochastická.
Výsledný stav tedy může být stejný pro stejný stav a akci.
  
\subsection{Odměna}\label{subsec:odmena}

  Odměna je hodnota, kterou agent obdrží od prostředí po vybrání akce.
  Může být kladná, záporná nebo nulová.
  Dle této zpětné vazby se agent učí, jak moc byla jeho zvolená akce v~daném stavu vhodná.

\subsection{Hodnotová funkce}\label{subsec:hodnotova-funkce}

  Hodnotová funkce vyhodnocuje, jak dobrý je stav tím, že predikuje budoucí odměnu, pokud bude další tah začínat v~tomto stavu.
  Čím vzdálenější odměna je od tohoto stavu, tím více je snížena.
  Protože, čím je odměna vzdálenější tím více je nejistá, že bude doopravdy získána.

  Existují dva typy hodnotových funkcí:

  \subsubsection*{Hodnotová funkce stavu $V(s)$}

  Hodnotová funkce stavu \emph{$V(s)$} vyhodnocuje očekávanou komulativní odměnu, jestliže se agent nachází v~tomto stavu.
  Tato funkce je závislá na strategii, kterou se agent řídí.
  Vyhodnocuje tedy jak příznivý je daný stav pro agenta.

  \subsubsection*{Hodnotová funkce akce $Q(s, a)$}
  \label{subsubsec:q_function}

  Hodnotová funkce akce \emph{$Q(s, a)$} vyhodnocuje očekávanou komulativní odměnu, pokud se agent nachází v~tomto stavu a zvolí tuto akci.
  Tato funkce je opět závislá na strategii, kterou se agent řídí.
  Vyhodnocuje tedy jak příznivé je zvolení dané akce v~aktuálním stavu.

\subsection{Markovský rozhodovací proces}\label{subsec:markovsky-rozhodovaci-proces}

Téměř všechny problémy, řešené posilovaným učením, mohou být označeny jako Markovy rozhodovací procesy (Markov Decision Process).
Tato abstrakce je základním kamenem pro modelování algoritmů posilovaného učení.
Markovský rozhodovací proces značí, že následující stav není závislý na stavech minulých, nýbrž pouze na aktuálním stavu.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{obrazky-figures/RL_basics}
	\caption{Interakce mezi prostředím a agentem podle Markova rozhodovacího procesu.
  Zdroj~\cite{RL_basics}:}\label{fig:markov}
\end{figure}

\begin{definition}
  
Markovský rozhodovací proces je definován pěticí $(S, A, P, R, \gamma)$~\cite{RL_basics}, kde:

\begin{itemize}
\item $S$ je množina stavů.
\item $A$ je množina akcí.
\item $P$ je pravděpodobnostní přechodová funkce
\item $R$ je odměnová funkce
\item $\gamma$ je diskontní faktor pro budoucí odměny
\end{itemize}
\end{definition}

\subsection{Bellmanovy rovnice}\label{subsec:bellmanovy-rovnice}

Belmanova rovnice se zaměřuje na rozložení hodnotových funkcí na menší a snadněji zpracovatelné celky.
Dociluje toho tak, že rozděluje hodnotovou funkci na dvě části: \emph{okamžitou odměnu} a postupně snižovanou \emph{budoucí odměnu} \cite{Dynamic_Programming_BELLMAN}.

\begin{equation}
  V(s) = E[R_{t+1} + \gamma V(S_{t+1}) | S_t = s]\label{eq:bellman1}
\end{equation}

Tato rovnice vyjadřuje hodnotu stavu $s$ jako očekávanou budoucí odměnu, kterou lze získat, začneme--li v~tomto stavu a budeme se řídit optimální strategií \cite{RL_basics,Dynamic_Programming_BELLMAN}.

\begin{equation}
  Q(s, a) = E[R_{t+1} + \gamma E_{a\sim\pi}Q(S_{t+1}, a) | S_t = s, A_t = a]\label{eq:bellman2}
\end{equation}

Tato rovnice vyjadřuje hodnotu akce $a$ ve stavu $s$ jako očekávanou budoucí odměnu, kterou lze získat, začneme--li v~tomto stavu, zvolíme akci $a$ a poté se budeme řídit optimální strategií \cite{RL_basics,Dynamic_Programming_BELLMAN}.

\begin{itemize}
  \item $V(s)$ je hodnota stavu $s$.
  \item $Q(s, a)$ je hodnota akce $a$ ve stavu $s$.
  \item $R_{t+1}$ je okamžitá odměna za provedení akce $a$ ve stavu $s$.
  \item $S_t$ je stav v~čase $t$.
  \item $A_t$ je akce v~čase $t$.
  \item $\pi$ je strategie.
\end{itemize}

Tyto rovnice jsou základem pro většinu algoritmů posilovaného učení \cite{RL_basics,Dynamic_Programming_BELLMAN}.

\subsection{Bellanovy rovnice optimality}\label{subsec:bellmanovy-rovnice-optimality}

\begin{equation}\label{eq:bellman_optimality1}
  Q_*(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{equation}

Tato rovnice udává, že hodnota akce $a$ ve stavu $s$ při optimální strategii je rovna okamžité odměně za provedení akce $a$ ve stavu $s$ plus diskontované hodnotě nejlepší akce ve stavu $s'$ při optimální strategii \cite{RL_basics,Dynamic_Programming_BELLMAN}.

\begin{equation}\label{eq:bellman_optimality2}
  V_*(s) = \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big)
\end{equation}

Rovnice \ref{eq:bellman2} udává, že maximální hodnota stavu $s$ při optimální strategii je rovna maximální hodnotě akce $a$ ve stavu $s$, pokud dále budeme pokračovat v optimální strategii \cite{RL_basics}.

\begin{itemize}
  \item $Q_*(s, a)$ je nejlepší možná hodnota akce $a$ ve stavu $s$ při optimální strategii.\cite{RL_basics}
  \item $V_*(s)$ je nejlepší možná hodnota stavu $s$ při optimální strategii.
  \item $R(s, a)$ je okamžitá odměna za provedení akce $a$ ve stavu $s$.
  \item $P_{ss'}^a$ je pravděpodobnost přechodu ze stavu $s$ do stavu $s'$ po provedení akce $a$.
  \item $\gamma$ je diskontní faktor pro budoucí odměny.
  \item $\mathcal{S}$ je množina stavů.
  \item $\mathcal{A}$ je množina akcí.
  \item $\max_{a' \in \mathcal{A}} Q_*(s', a')$ je maximální hodnota akce ve stavu $s'$ při optimální strategii.
\end{itemize}    


\subsection{Rovnováha mezi explorací a exploatací}
\label{subsec:exploration-exploitation-dillemma}
\begin{quote}
  \emph{Kompromis mezi potřebou získávat nové znalosti a potřebou použít již nabyté znalosti k~vylepšení výkonnosti je jedním z~nejzákladnějších kompromisů v~přírodě~\cite{Exploitation_Exploration}.}
\end{quote}
Explorace a exploatace jsou dvě protichůdné strategie, které se vyskytují jak ve strojovém učení, tak i v~reálném životě.

\emph{Exploatace} se snaží vybrat nejlepší možnou akci na bázi známých informací.
Tyto informace, nemusí být kompletní, nebo mohou být zavádějící.
A~to z~důvodu nedostatečného trénování, či nedostatečného prozkoumávání možností prostředí.

Tomu opačná metoda \emph{explorace} usiluje o~prozkoumání možností, které nejsou známé a mohly by vést k~lepší budoucí odměně.
Explorace tedy často zvolí akci, která nemusí být nejlepší, ale může odhalit nové informace, které následovně povedou ke zlepšení exploatace.

\subsection{Druhy informací v~teorii her}\label{subsec:druhy-informaci-v-teorii-her}
V~rámci umělé inteligence se potýkáme s~různými druhy informací.
Dělí se na tyto hlavní typy:

\textbf{\emph{Dokonalá informace}} znamená, že agent ví o~prostředí a o~ostatních hráčích vše.
Například ve hře šachy.
Hráč vidí všechny figury na herní ploše, i ty soupeřovy.

\textbf{\emph{Kompletní informace}} značí, že agent je obeznámen se strukturou hry a jsou mu také odhaleny odměnové funkce ostatních hráčů.
Hráč tedy ví, jakou hru hraje je obeznámen s~jejími pravidly.
A~rozumí, jaké jsou podmínky výhry a je obeznámen s~taktikou ostatních hráčů.

\textbf{\emph{Nedokonalá informace}} znamená, že agent nemá všechny relevantní informace o~prostředí a ostatních hráčích.
Například, tedy všechny hry, ve kterých hrají hráči zároveň jsou hry s~nedokonalou informací.
Jelikož hráč v~daném okamžiku nezná informaci o~tahu ostatních hráčů.
Další příklad je například hra poker, kde hráč nezná rozdané karty ostatních hráčů.
Také hra Scotland Yard, kde policisté neznají pozici Pana~X\@.

\textbf{\emph{Neúplná informace}} znamená, že hráč nezná strukturu odměn, podstatu hry nebo její pravidla.
Hráč tedy nezná výchozí informace o~hře.
Všechny hry s~neúplnou informací se dají považovat za hry s~nedokonalou informací.

\emph{Soukromá informace} je informace, která není dostupná ostatním hráčům.

\emph{Společná informace} je informace, která je dostupná všem hráčům.

\section{Vhodné algoritmy k~řešení her s~nedokonalou informací}\label{sec:vhodne-algoritmy-k-reseni-her-s-nedokonalou-informaci}

Tato kapitola se zaměřuje na algoritmy, které jsou vhodné pro řešení her s~nedokonalou informací, s~důrazem na metody posilovaného učení a srovnáním s~klasickými metodami jako Monte Carlo.

\subsection{Monte Carlo tree search}\label{subsec:monte-carlo-tree-search}
Metoda Monte Carlo tree search (MCTS) je heuristický algoritmus prohledávání.
Kombinuje stromové vyhledávání s~principy posilovaného učení.
Je často využíván, je--li stavový prostor řešeného problém příliš velký a složitý na to, aby byl prohledán kompletně jinými metodami, jako například minimax, či alfa--beta prořezávání.
Tyto \uv{tradiční} algoritmy nelze na mnoho problémů použít, jelikož by byly příliš pomalé a náročné na výpočet.

Tato metoda se také potýká s~rovnováhou mezi explorací a exploatací (viz.~ sekce~\nameref{subsec:exploration-exploitation-dillemma}).
Explorací se strom rozrůstá do šířky, zatímco exploatací se strom prohlubuje~\cite{mcts_geeksforgeeks}.

MCTS se skládá z~několika fází:
\begin{itemize}
  \item \emph{Selekce}

  Na základě aktuálního stavu se vybere další stav k~prozkoumání.
  Pro tento výběr se využívají dvě strategie:
  
  \emph{Strom s~horní mezí spolehlivosti} (Upper confidence bounds applied to trees, UCT) kombinuje průměrnou hodnotu uzlu a odměnu za exploraci.

  \label{epsilon_greedy}
  \emph{Chamtivá strategie $\epsilon$ } ($\epsilon--greedy strategy$) vybírá s~pravděpodobností $\epsilon$ náhodný uzel, jinak volí uzel s~nejvyšší hodnotou.
  Tato strategie se používá méně často než UCT\@.

  Obě tyto strategie se snaží o~rovnováhu mezi explorací a exploatací.
  \item \textbf{\emph{Expanze}}

  V~tomto kroku se vyhledávací strom rozšíří o~nový uzel, který je výstupem z~předchozího kroku.

  \item \textbf{\emph{Simulace}}

  Po této fázi je provedena náhodná simulace od nového uzlu až do konečného stavu.

  \item \textbf{\emph{Aktualizace}}
  
  Díky nově nabitým informacím ze simulace, se zpětnou propagací aktualizují hodnoty uzlů ve stromě.

\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{obrazky-figures/mcts}
	\caption{Diagram jednotlivých fází MCTS.\@
  Zdroj: \cite{mcts_geeksforgeeks}}\label{fig:mcts}
\end{figure}

Tento algoritmus se skvěle hodí na hry s~nedokonalou či neúplnou informací, jelikož se spoléhá na vzorkování pomocí simulací.

\subsection{Q-learning}\label{subsec:q-learning}

Q-learning je jedním z~nejznámějších algoritmů posilovaného učení.
Učení tohoto algoritmu probíhá bez modelu a mimo strategii.

Jak už název vypovídá, Q-learning se zaměřuje na určení hodnoty hodnotové funkci akce $Q(s, a)$, viz. \nameref{subsubsec:q_function}.
Tyto hodnoty se uchovávají v~Q--tabulce, která na každou kombinaci stavu a akce uchovává hodnotu $Q(s, a)$.
Hodnoty v~Q--tabulce se iterativně aktualizují na základě získaných odměn.
Kladná odměna po vykonání akce $s$, zvýší se hodnota $Q(s, a)$.
Naopak, dostane--li po vykonání této akce zápornou odměnu, hodnota $Q(s, a)$ se sníží.

Řádky Q-tabulky tedy reprezentují stavy a sloupce akce.
Po vytvoření jsou všechny $Q$ hodnoty v~tabulce inicializovány na nulu.
Následně jsou tyto hodnoty iterativně aktualizovány dle zpětné vazby udělené od prostředí ve formě odměny.

Jako u~většiny algoritmů je zde potřeba dohlédnout na rovnováhu mezi explorací a exploatací.
K~tomuto se využívá $\epsilon$--greedy strategie viz. \nameref{epsilon_greedy}.

Hlavní nevýhodou tohoto algoritmu je právě jeho závislost na Q--tabulce.
Tabulka mapuje hodnoty pro každou kombinaci stavu a akce,
Je--li avšak stavový, či akční prostor příliš velký, nebo dokonce nekonečný je tento algoritmus nevhodný, skoro až nepoužitelný.
Výsledná Q--tabulka by byla neefektivní kvůli své velikosti.
Mohlo by se stát, že s~nekonečným množstvím kombinací by velikost tabulky rostla do \uv{nekonečna}.
Jako řešení byl navrhnut algoritmus Deep Q-learning.

\subsection{Deep Q-learning (DQN)}\label{subsec:deep-q-learning}

Tato metoda je rozšířením algoritmu Q-learning které,
nahrazuje $Q$--tabulku neuronovou sítí určených k~aproximaci hodnotové funkce akce $Q(s, a)$.
Díky této aproximaci je možné použít tento algoritmus na problémy s~velkým, či nekonečným množstvím kombinací akcí a stavů.

Avšak tímto vzniká nový problém, \emph{nestabilita učení}.
Ten je řešen dvěma mechanizmy:

\begin{itemize}
  \item \textbf{\emph{Přehrání zkušenosti (experience replay)}}
  
  Během trénování se ukládají všechny zkušenosti do paměti.
  Ať už to jsou akce, stavy, odměny atd.
  Při trénování se poté náhodně vybírají náhodně zkušenosti z~této paměti a aktualizují se podle nich váhy sítě.
  Tímto se snižuje rozdíl mezi jednotlivými aktualizacemi a tím pádem se zvyšuje stabilita učení.
  

  \item \textbf{\emph{Periodická aktualizace}}
  Síť je naklonována a změny se provádí pouze na duplikátní verzi.
  Do hlavní originální sítě se změny klonují po určitém počtu kroků.

\end{itemize}

Při vytváření neuronové sítě jsou váhy inicializovány náhodně.

\subsection{Gradient strategie}\label{subsec:gradient-strategie}
Oproti předchozím zmiňovaným algoritmům, které usilují o~naučení hodnotové funkce či prohledávají stavový prostor, algoritmy gradientu strategie se snaží naučit strategii přímo.

V~diskrétním prostoru je odměnová funkce definována jako:
\begin{equation}
  \label{eq:odmenova_funkce}
  \mathcal{J}(\theta) = V_{\pi_\theta}(S_1) = \mathbb{E}_{\pi_\theta}[V_1]
\end{equation}

V~spojitém prostoru je odměnová funkce definována jako:
\begin{equation}
    \label{eq:odmenova_funkce_spojita}
  \mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) V_{\pi_\theta}(s) = \sum_{s \in \mathcal{S}} \Big( d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s, \theta) Q_\pi(s, a) \Big)
  \end{equation}

\subsubsection*{Věta o~gradientu strategie}
Výpočet gradientu matematicky je zaznamenán touto rovnicí:

\begin{equation}
  \label{eq:gradient_strategie}
  \frac{\partial \mathcal{J}(\theta)}{\partial \theta_k} \approx \frac{\mathcal{J}(\theta + \epsilon u_k) - \mathcal{J}(\theta)}{\epsilon}
\end{equation}
Tento výpočet je velmi pomalý a náročný na výpočet.
Avšak tento vzorec lze zjednodušit na rovnici zvanou \emph{věta o~gradientu strategie}:

\begin{equation}
    \label{eq:veta_o_gradientu_strategie}
  \nabla \mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s, \theta) Q_\pi(s, a)]
\end{equation}

\section{Trust Region Policy Optimization (TRPO)}\label{sec:trust-region-policy-optimization}
Trénované strategie jsou často velmi náchylné na změny.
Kde i malá náhlá změna v~jednom kroku může způsobit velké změny v~chování agenta a zamezit dalšímu učení správné strategie.
Během učení totiž chceme, aby učení probýhalo plynule a ne skokově.
Pokud se strategie změní příliš rychle protože následovala nějvětší růst, může se stát, že mine cestu vedoucí k~optimální strategii.
Uváznutí se tedy v~lokálním optimu.\pagebreak

TRPO je optimalizační algoritmus, který se snaží tento problém řešit tím, že definuje omezení rozdílu mezi novou aktualizovanou strategií $\textbf{p}$ a starou strategií z~předešlého kroku $\textbf{q}$.
Tento rozdíl mezi dvěma pravděpodobnostními rozděleními je definován jako Kullback-Leiblerova divergence (K-L divergence)\cite{KL_divergence}.
Vzorec pro K-L divergenci ve spojitém prostoru je následující:
\begin{equation}
  D_{KL}(p \| q) = \int_x p(x) \log \frac{p(x)}{q(x)} dx
\end{equation}

Vzorec v diskrétním prostoru:
\begin{equation}
  D_{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)}
\end{equation}

Vzniká tak region důvěry, ve kterém musí nová strategie setrvat.
strategie se tedy nemůže skokově měnit na lokální maximum, ale pouze následuje směr lokálního maxima.

K~tomu využívá Hesenská matice.
Kdy pomocí těchto matic hledá optimální směr změny strategie.
Tím, že se hledá lokální maximum pouze v~oblasti důvěry, řeší problém se zaseknutím v~těchto lokálních optimech.
Hesenská matice je vždy čtvercového tvaru.
Její hodnoty obsahují druhé parciální derivace funkce a popisuje tím její lokální zakřivení.
Jelikož je strategie pravděpodobnostní funkce, jsou tyto matice ideální pro hledání optimálního směru změny strategie.

Díky tomuto řešení se učení stává robustním a stabilním.
Avšak výměnou za to, je náročnost aproximace pomocí Hesenských matic a s~tím související náročnost na implementaci.
\begin{equation}
  H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
\end{equation}
Kde $i$ značí řádek a $j$ sloupec matice.
Výsledná matice tak poté může vypadat takto:

\begin{equation}
  \begin{bmatrix}
     \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\
     \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2}
   \end{bmatrix}
\end{equation}

\section{Proximální optimalizace strategie (PPO)}
\label{sec:proximalni-optimalizace-strategie}

%\textsuperscript {\ref{sec:trust-region-policy-optimization}}

Metoda proximální optimalizace strategie (Proximal policy optimization - PPO) byla představena roku 2017\cite{PPO_paper}.
Tato metoda je vylepšeným následníkem algoritmu TRPO, popsaný v~předchozí sekci~\ref{sec:trust-region-policy-optimization}.

Co výrazné odlišuje tuto metodu od předchůdců, je její jednoduchost a efektivita.
Dosahuje lepších výsledků než metoda TRPO a srovnatelných výsledků s~metodou ACER\@.
Je avšak mnohem jednodušší na implementaci, má nižší nároky na výkon a je mnohem lepší co se týče efektivity dat pro trénování.\cite{PPO_paper}.
K~dosažení těchto výsledků, využívá PPO techniku ořezávání náhradních cílů vloženou do algoritmu TRPO\@.
Vynucuje, aby K-L divergence staré a nové strategie byla v~rozmezí $[1-\epsilon, 1+\epsilon]$ kde $\epsilon$ je modifikovatelný parametr.

Nechť $r_t(\theta)$ je pravděpodobnostní poměr mezi novou a starou strategií.
\begin{equation}
  r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{\theta_{old}}(a_t \vert s_t)}
\end{equation}

Pokud tento vzorec dosadíme do rovnice pro výpočet gradientu strategie TRPO. Kde TRPO využívá K-L divergenci a maximalizuje vedlejší cíle \cite{policy_gradients}, vznikne následnovná rovnice:

\begin{equation}
  L^{CLIP}(\theta) = \mathbb{E}_t \Big[ \min \Big( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \Big) \Big]
\end{equation}

\begin{equation}
  L^{CLIP}(\theta) = \mathbb{E}_t \Big[ \min \Big( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \Big) \Big]
\end{equation}

\begin{itemize}
  \item $\theta$ je parametr strategie.
  \item $\mathbb{E}_t$ značí empirickou očekávání v časových krocích
  \item $r_t$ relativní podobnost nové a staré strategie \cite{PPO_paper}
  \item $\hat{A}_t$ je odhad výhody akce v~čase $t$.
  \item $\epsilon$ je hyperparametr, obvykle nastaven na hodnotu $0.1$ nebo $0.2$.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{obrazky-figures/clip}
	\caption{Grafy zobrazují 1 časový krok funkce $L^{CLIP}$. Červeným bodem je označen počáteční stav omptimalizace. Levý graf zobrazuje kladnou změnu a pravý zápornou změnu.
  Zdroj: \cite{RL_basics}}\label{fig:clip}
\end{figure}

Tyto změny tvoří PPO oproti TRPO kompatibilní s metodou stochastického gradientního sestupu.
Odstraňujě KL omezení a nahrazuje je ořezáváním.

PPO má však i své nedostatky.
\label{PPO_weakness}
Dle studie~\cite{PPO_weakness} se ukázalo, že PPO nefunguje optimálně za 3 podmínek:
\begin{enumerate}
  \item V~prostředí se spojitým prostorem akcí je nemodifikované PPO nestabilní, pokud odměna náhle zmizí mimo ohraničenou podporu.
  \item V~diskrétním akčním prostoru s~řídkými a vysokými odměnami PPO volí neoptimální akce.
  \item V~době těsně po inicializaci je náchylné k~předčasné volení strategie, pokud je některá z~optimálních akcí po inicializaci blízko a snadno dosažitelná.
\end{enumerate}

Tato studie také navrhla tyto řešení a prokázala jejich účinnost.
Bod 1 a 3 je řešen buď převedením spojitého akčního prostoru na diskrétní nebo zavedením \emph{Beta parametrizace strategie}.
\emph{KL regulováním cíle} je řešen bod 2.

Podmínka číslo 1 je v~mé implementaci prostředí hry Scotland Yard implicitně neplatná, jelikož akční prostor je diskrétní.\\
Podmínka číslo 2 je řešena přídáním menších dílčích odměn.
Agenti tak získávají odměny i za menší kroky, nejen například za vítězství/ prohru či blízkost, viz. podsekce~\nameref{subsubsec:odmeny}.\\
\subsection{Experiment OpenAI}\label{subsec:experiment-openai}
Důležitým zdrojem pro tuto práci byla studie studie~\cite{PPO_Hide_Seek_paper}, od společnosti OpenAI, vyplývá, že algoritmus je vhodný pro řešení problémů s~nedokonalou informací.
V~dané studii byl algoritmus implementován na komplexní hru schovávané.
Ve hře proti sobě hrají dva typy hráčů.
Modří hráči se snaží schovat a utéct před červenými.
Po herní ploše byly rozestavěny objekty, se kterými hráči mohou interagovat.
Mohou je přesouvat a následně \uv{zamrazit} na místě, takže s~objektem nelze pohybovat.
Modří hráči se na začátku hry objevují ve své pevnosti, která má několik děr.
Červeným hráčům je na začátku hry znemožněn pohyb, což dává modrým hráčům čas připravit se na jejich útok.

Po více než 8 miliónech epizodách učení se modří hráči naučili efektivně blokovat vstup do jejich pevnosti, takže je červení hráči nebyli schopni dostihnout.
Poté se, avšak červení hráči adaptovali, naučili se totiž využívat rampy a přelézt opevnění modrých hráčů.
Jako finální, a ultimátní strategii se modří naučili po 43 miliónech epizod.
Modří hráči na začátku kola ukradli červeným všechny rampy, a zabarikádovali se i s~nimi v~pevnosti.
Červení hráči tak neměli žádnou šanci na výhru.

Dokonce se objevili i fascinující strategie které zneužívaly prostředí.
Například, v~kódu obsluhující kolize byla chyba, která způsobovala, že při najetí rampy na zeď arény, pod určitým úhlem, byla rampa rapidní rychlostí vymrštěna do vzduchu.
Toho červení hráči zneužili a za pomocí této chyby se vymršťovali do vzduchu, aby překonali zdi pevnosti modrých.

\chapter{Zhodnocení současného stavu a plán práce (návrh)}
\label{ch:navrh}
\begin{itemize}
  \item \emph {Kritické zhodnocení dosavadního stavu}
  \item \emph {Návrh, co by bylo vhodné vyřešit na základě znalostí dosavadního stavu}
  \item \emph {Co jste konkrétně udělal s~teorií popsanou výše}
  \item \emph {Volba OS, jazyk, knihovny}
  \item \emph {Detailní rozbor zadání práce, detailní specifikace a formulace cíle a jeho částí}
  \item \emph {Popis použití řešení, situace/problémy, které projekt řeší}
  \item \emph {Postup práce/kroky vedoucí k~cíli, rozdělení celku na podčásti}
  \item \emph {Návrh celého řešení i jeho částí, s~odkazy na teoretickou část}
\end{itemize}

\section{Zkoumaná modifikovaná verze hry Scotland Yard}\label{sec:zkoumana-modifikovana-verze-hry-scotland-yard}

Tato práce využívá modifikovanou verzi hry Scotland Yard, ve které se hráči pohybují po mřížkové herní ploše ve tvaru čtverce.
Na mřížce se nachází 15$\times$15 polí.
Hráčí se po těchto polích mohou pohybovat ortogonálně i diagonálně, vždy o~však maximálně 1 pole.
Hráč se může rozhodnout nezměnit pozici a zůstat na svém aktuálním poli.
K~pohybu nejsou potřebné žádné jízdenky.

Předtím nežli začne hra, se vyberou náhodné startovací pozice Pana~X a policistů.
Z~těchto možných pozic se následně náhodná pozice přidělí jednotlivým hráčům.
Poté začíná hra.
Hra se dělí na jednotlivá kola, ve kterých se hráči ve svých tazích střídají.
Pro hru byli zvoleni 3 policisté z~toho důvodu, že je herní pole velké a 2 policisté by nemuseli mít možnost ho celé pokrýt.
V~kole hraje jako první Pan~X a poté policisté, již podle jejich očíslování, které jim bylo náhodně přiděleno při vytváření.
Hra končí v~okamžiku, kdy policisté chytí Pana~X nebo když Pan~X zůstane nepolapen až do konce.

Z~původní verze hry byly odstraněny některé elementy jako jsou jízdenky a různé druhy dopravních prostředků.
Tato úprava vede k~tomu, že se zjednodušila strategie agentů, jelikož se nemusí starat o~svoje jízdenky a mohou se pohybovat po herní ploše libovolně.
Změny, avšak nemění základní podstatu hry, zachovává neurčitost, ale značně zjednodušuje implementaci.
Stavový prostor je tedy zjednodušený a tím se i snižují nároky na výkon.

\section{Implementace modelu a agentů do hry Scotland Yard pomocí algoritmu PPO}
\label{sec:implementace}

\subsection{Použité technologie}\label{subsec:pouzite-technologie}
K~učení strategie pro hru Scotland Yard byl využit open-source framework \texttt{Ray}~\cite{Ray}.
Konkrétně byla využita knihovna \texttt{Ray.Rllib}.
Tato knihovna poskytuje nástroje pro posilované učení a samotné implementace jednotlivých algoritmů, včetně zkoumaného algoritmu \emph{PPO}.
Rllib dokáže využívat obě populární knihovny pro strojové učení \texttt{Tensorflow} a \texttt{Pytorch}.
Pro tuto práci byla vybrána knihovna \texttt{Pytorch}.

Před finálním rozhodnutím pro využití frameworku \texttt{Ray} byly zkoumány i další možnosti.
Knihovna Stable Baselines 3, která byla zavrhnuta, jelikož neumožňuje učení více strategií souběžně při hraním agentů proti sobě.
Knihovna \textit{CleanRL} byla také zavrhnuta, jelikož nemá dostatečně implementované trénování více strategií souběžně.

\subsection{Implementace uživatelského rozraní a herních mechanizmů}\label{subsec:implementace-uzivatelskeho-rozrani-a-hernich-mechanizmu}

Uživatelské rozhraní bylo vytvářeno pomocí knihovny \texttt{Pygame}.
Hra začíná v~menu, kde je momentálně možné vybrat pouze možnost sledování hry mezi dvěma agenty.
Je zde ale možnost vybrat jaký algoritmus rozhodování je použit pro jednotlivé agenty.
K~dispozici jsou algoritmy \emph{PPO}, \emph{DQN} a náhodné chování.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{obrazky-figures/game_title}
	\caption{Menu hry}\label{fig:game_title}
\end{figure}

Samotný kód hry je rozdělen na 3 částí.
Jednotlivé vrstvy hry jsou tak izolovány a mohou být snadno vyměněny za jiné implementace.

\begin{itemize}
  \item \emph{GameController}~\cite{GameSceneController}
  
  Tato třída je zodpovědná za řízení hry.
  Je zde spuštěna univerzální herní smyčka, která zpracovává uživatelské vstupy.
  A~následně provádí aktualizaci stavu aktuální scény a překreslení dané scény.
  \item \emph{Scény}~\cite{GameSceneController}
  
  Jednotlivé scény následně definují své chování při aktualizaci a překreslení.
  Manipulace a přepínání mezi nimi je zajištěno pomocí zásobníků scén.
  Do tohoto zásobníku se ukládají nově otevřené scény a obsluhována je vždy ta nejnovější.

  \item \emph{Hra}

    Samotná hra je následovně rozdělena na 2 další části která každá zpracovává jinou část hry.\vspace{-0.5em}
    \begin{itemize}
      \item \emph{Herní logika} --- src/game/scotland\_yard\_game\_logic.py
      Zpracovává herní mechanizmy, jako je pohyb, zpracování výherních podmínek atd.
      Zprostředkovává informace pro prostředía a to poté pro učící se agenty.

      \item \emph{Herní vizualizace} ---  src/game/scotland\_yard\_game\_visual.py
      Vykreslování herních elementů (herní pole, figury atd.).
    \end{itemize}
\end{itemize}

\textit{Použití herní smyčky, která obsluhuje aktuální scénu bylo inspirován výukovým projektem z~platformy GitHub~\cite{GameSceneController}.
Kód byl, avšak značně upraven a vylepšen, aby vyhovoval integrování do této práce.}

Hra se zapne v~pozastaveném stavu, aby bylo možné lépe pozorovat počáteční rozložení hráčů.
Pokud je hra pozastavena, je možné pokračovat stisknutím klávesy Space (mezerník).
Jakmile hra běží, je možné ji opětovně pozastavit stisknutím klávesy Space (mezerník).

Samotná hra je zobrazena na herní ploše, kde se nachází 3 policisté a Pan~X\@.
Je rozdělena do mřížky 15$\times$15 polí.
V~mřížce jsou zobrazeny 4 figury ve formě barevného čtverce.
Červený čtverec značí Pana~X a zelené čtverce značí policisty.
V~levém horním rohu okna je zobrazen aktuální stav hry.
Pakliže hra stále běží, je zobrazeno číslo aktálního kola.
Pokud hra skončila, je zobrazeno, kdo hru vyhrál.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{obrazky-figures/game_0}
    \caption{Hra před spuštěním}
    \label{fig:game_0}
\end{figure}

V~obrázku~\ref{fig:game_0} je hra v~pozastaveném stavu a je zobrazeno počáteční rozložení hráčů.
Na obrázku lze vidět tvavě šedé čtverce, které značí možnou pozici Pana~X\@.
Na začátku hry, je těchto pozic mnoho, jelikož Pan~X doposud svoji pozici neodhalil.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{obrazky-figures/game_11}
      \caption{Jedenácté kolo hry}
    \label{fig:game_11}
\end{figure}

Na obrázku~\ref{fig:game_11} je zobrazeno jedenácté kolo hry.
Světlé šedý a nejvýraznější čtverec značí poslední známou pozici Pana~X\@.
Tmavě šedé čtverce opět značí možné pozice Pana~X\@.
Na tomto obrázku lze pozorovat, že policisté jsou v~blízkosti Pana~X a snaží se ho chytit.
Všichni policisté se z~pozic, zobrazených na obrázku~\ref{fig:game_0}, přesunuli na možné pozice Pana~X\@.

\subsection{Prostředí a učení}\label{subsec:prostredi}

Vytvořit prostředí pro platformu \texttt{Ray.Rlib} bylo složité.
Dokumentace vytváření prostředí není příliš komplexní, byl tedy problém zajistit správnou komunikaci prostředí a agentů.
Prostředí a následné učení je komplikované, jelikož zde figuruje více agentů a naráz se učí dvě rozdílné strategie.

Agenti se již od začátku učí zásadně pouze hraním proti sobě.
Důvodem pro toto rozhodnutí bylo, že se jedná o~zajímavější experiment, kdy jsou agenti vpuštěni do zcela neznámého prostředí a nemají se od koho učit strategii.
Jejich učení tedy není nijak podporováno a musí se naučit strategii sami.
Tím je i pozorováno, jestli jsou agenti vůbec schopni rozpoznat cíl hry, co je po nich požadováno a případně jak rychle k~dosáhnou dobré strategie.

Implementace algoritmu PPO i DQN je obsažena v~knihovně \texttt{Ray.Rlib}.
PPO využívá základní parametry trénování bez žádné modifikace parametrů jelikož se ukázalo, že agenti se chovají optimálně a hru pochopili i bez těchto zásahů do výchozích hodnot hyperparametrů.
Zato DQN agenti se chovali \emph{ne}optimálně.
Agenti se naučili pouze stát na místě, přestože tato taktika byla velmi nevýhodná.
Pro vyřešení tohoto problému, je nyní agentům udělována penalizace pokud se 5 kroků po sobě nehnou.
Penalizace je rovna zápornému počtu kol nečinnosti.
Postupně tedy roste a pohybem je resetována.
Tato expanze odměn vedla k vylepšení strategie agentů DQN a nadále nestáli pouze na jednom místě.
Bohužel však přes veškeré snahy, agenti DQN nedosahují tak dobrých výsledků jako agenti PPO\@.

S konfiguračními parametry \texttt{Ray.rlib}u a hyperparametry DQN jsem dlouho experimentoval.
Měnila se velikost vnitřních vrstev, počet neuronů, rychlost učení, velikost paměti, velikost dávky učení a další.

Výsledná konfigorace je následující.
Obsahuje 2 vnitřní skryté vrstvy, první vrstvá má 128 neuronů a druhá vrstva má 64 neuronů.
Používá verzi \emph{double DQN}
Algoritmus k~vyvážení explorace a exploatace při učení využívá metodu $\epsilon$--greedy, kde je $epsilon$ postupem trénování lineárně snižováno z~hodnoty 1 na 0.05.
Tato hodnota udává pravděpodobnost náhodné volby akce.
Po natrénování modelu je ve hře explorace vypnuta, $epsilon$ je tedy nastaveno na 0.
K posílení explorace během trénování je k vahám sítě přidán \uv{šum}.
Avšak i přesto jsou výsledky agentů DQN neuspokojivé, viz.\nameref{ch:experimenty}.

\subsubsection*{Řešení zvolení nevalidních akcí}

Hra Scotland Yard má omezené herní pole.
Tím pádem je jasné, že pokud se hráči vyskytují na okraji herního pole, nemohou zvolit takovou akci, která by je posunula mimo toto herní pole.
Policisté také nemohou zvolit akci, která by je posunula na stejné pole, ve kterém již je jiný policista.
K~vyřešení tohoto problému se nejčastěji využívá tzv.~ maska akcí (\emph{Action mask}).
Framework Ray tuto možnost v~omezené míře podporuje.
Vytvořené prostředí je, avšak potřeba obalit v~obalové třídě, která tuto funkcionalitu zprostředkuje.
Bohužel se mi nepodařilo tuto funkcionalitu spojit s~dalšími požadavky systému, které také vyžadují zabalení do jiné třídy.
Mezi těmito požadavky je fungování více aktérů s~různými strategiemi v~jednom prostředí a různé pozorovací prostory agentů.

strategie, tedy může zvolit nevalidní akci, ale je za jejich zvolení velmi penalizována.
Agenti ji tedy volí opravdu zřídka, a to jen na základě velmi malé explorační šance a pouz na počátku trénování.
Pokud se i tak stane, je ve hře implementovaná funkcionalita, která se pokusí znovu vygenerovat další akce, dokud vygenerovaná akce není validní.
Aby se zamezilo nekonečnému cyklu čekání na validní akci, je po 100 pokusech vygenerována náhodná validní akce.
Takto uměle generovaná akce, nebyla za celou dobu testování agentů PPO potřeba, avšak ze statistického hlediska může tato situace nastat.

Zavedení tohoto omezení pomocí systému odměn vedlo k~positivním výsledkům a agenti se naučili jaké akce jsou nevalidní a nepoužívají je.

\subsubsection*{Systém odměn}
\label{subsubsec:odmeny}

Jednotlivým agentům jsou udělovány odměny na základě jejich chování.
Jelikož policisté a Pan~X mají odlišné a protichůdné strategie i jejich odměny jsou odlišné.
Odměny které agenti dostávají byly navrženy odhadem a laděny postupným experimentováním.
Konečné odměny jsou vypočítány následovně:

\begin{equation}
    \label{eq:odmena_za_vzdalenost_od_policisty}
  R_{\rho_p} = \rho_{p_c} - \rho_{pref}
\end{equation}
\begin{equation}
    \label{eq:odmena_za_vzdalenost_od_posledni_pozice}
  R_{\rho_l} = \rho_l * 0.2
\end{equation}
\begin{equation}
    \label{eq:celkova_odmena}
  R = R_{\rho_p} + R_{\rho_l}
\end{equation}

\begin{itemize}
  \item $R_{\rho_p}$ - odměna za vzdálenost od policistů
  \item $R_{\rho_l}$ - odměna za vzdálenost Pana~X od jeho poslední známé pozice
  \item $R$ - celková odměna
  \item $\rho_l$ - vzdálenost Pana~X od jeho poslední známé pozice
  \item $\rho_{p_c}$ - vzdálenost nejbližšího policisty od Pana~X
  \item $\rho_{pref}$ - pomezí mezi kladnou a zápornou odměnnou.
  Pokud je vzádelenost od policisty menší než tato hodnota, Pan~X obdrží záporno odměnu a naopak kladnou.
\end{itemize}

Bylo také experimentováno s variantou kde byla odměna za vzdálenost od policistů počítána pro všechny policisty zvlášť a následně byla zprůměrována:
$  R_{\rho_p} = \sum_{i=1}^{3}[(\rho_{p_i} - \rho_{pref})] \div 3$
Což ale vedlo k tomu, že Panu X tolik \uv{nevadilo} když byl blízko jednoho policisty, ale daleko od zbylých dvou.
Výpočet této odměny se tedy změnil na výše uvedený vztah~\ref{eq:odmena_za_vzdalenost_od_policisty}.
Což vedlo ke zlepšení výsledků Pana X.

Výpočet odměny policistů je složitější.
Policisté znají poslední odhalenou pozici Pana~X\@.
Okolo tohoto bodu je vytvořená oblast zájmů, ve které se Pan~X může nacházet.
Policisté znají bod této oblasti, který je jim nejbližší, neboli vstupní bod.
Čím blíže je policista k~tomuto bodu, tím větší odměnu získává.
Pokud je od oblasti velmi vzdálený obdrží penalizaci.
Jakmile policista docílí oblastí zájmů dostává konstantní odměnu za to, že se pohybuje v lokaci ve které se může objevovat Pan X.

PPO algoritmus může začít v~prostředích s~diskrétními akcemi volit velmi neoptimální akce.
Toto nastává pokud má příliš málo podstatných odměn\cite{PPO_weakness}.
To se dokonce ukázalo i během tohoto experimentu, kdy v~raných fázích vývoje nebyla udělována odměna za vzdálenost od oblasti zájmu.
Učení nebylo příliš úspěšné, ale přídáním této dílčí odměny za interakci s~touto oblastí se výsledná strategie značně zlepšila a agenti dosahovali lepších výsledků.

Tento styl odměňování policistů se ukázal jako velmi efektivní.
Další varianta byla přidat do pozorování policistů pozice všech pozic, kde se může Pan~X nacházet.
Toto by ale bylo méně efektivní, jelikož by se pozorovací prostor několikanásobně zvětšil.
Tím by se zvýšila náročnost výpočtů a zpomalilo by se učení.
\subsubsection*{Pozorovací a akční prostor agentů}

\textbf{Pozorovací prostor} agentů je doopravdy velmi důležitý pro úspěšné naučení dobré strategie.
V~této práci vzájemně v~prostředí vystupují dva agenti s~různými strategiemi a různými pozorovacími prostory.
Akční prostor je stejný jak pro policisty, tak pro Pana~X\@.

Struktury jednotlivých pozorovacích prostorů jsou následující:

\begin{itemize}
  \item Shodné položky
    \begin{itemize}
    \item Pozice X, Y sama sebe
    \item Pozice X, Y a vzdálenost od poslední známé pozice pana~X
    \item Číslo aktuálního kola
    \item Maximální počet kol
    \item Číslo kola, kdy dojde k~dalšímu odhalení pozice Pana~X
  \end{itemize}
  \item \textbf{Pan~X}
    \begin{itemize}
    \item Pozice X, Y policistů
    \item Vzdálenost od  policistů
  \end{itemize}
  \item \textbf{Policisté}
    \begin{itemize}
    \item Pozice X, Y zbylých policistů
    \item Vzdálenost od zbylých policistů
    \item Pozice X, Y nejbližšího bodu oblasti zájmu
    \item Vzdálenost od nejbližšího bodu oblasti zájmu
    \item Pravdivostní hodnota, zda je policista v~oblasti zájmu
  \end{itemize}
\end{itemize}

Velikost pozorovacího prostoru tedy není příliš velká a je snadněji zpracovatelná, než kdyby obsahoval všechny herní políčka.
Celkově má tedy pozorovací prostor Pana~X 17 rozměrů a pozorovací prostor policistů 18 rozměrů.
Tento prostor je diskrétní s~typovou hodnotou \emph{numpy.float32}, jelikož se v~něm vyskytují hodnoty vzdálenosti a ty nejsou celočíselné.

Pokud informace obsažená v~těchto pozorovacích prostorech není v~daný moment definována, je nahrazena hodnotou -1.
Konkrétně se jedná pouze o~hodnoty týkající se poslední veřejně známé pozice Pana~X, jelikož v~prvních několika kolech není známa.

\bigskip

\textbf{Akční prostor} agentů reprezentuje možné akce, které mohou agenti provést.
Tyto akce jsou stejné jak pro policisty, tak pro Pana~X\@.
Jedná se ortogonální i diagonální pohyb.
Agent také může zůstat na místě a neprovést žádnou akci.
Celková velikost akčního prostoru je tedy 9.
Jedná se o~diskrétní prostor, jehož výstupem je hodnota 1 má--li být akce zvolena.

\chapter{Experimenty}
\label{ch:experimenty}

Pro ověření vlastností algoritmu PPO byly provedeny dva experimenty.
Experimenty byly prováděny na systému s~následujícími parametry:

\begin{itemize}
  \item Operační systém: Windows 10 (64-bit)
  \item Procesor: Intel Core i5--9300H
  \item Paměť: 16 GB DDR4
  \item Grafická karta: NVIDIA GeForce GTX 1660 Ti
\end{itemize}

Agenti DQN i PPO se učili ve stejném prostředí, za stejných počátečních podmínek, měli totožný stavový i akční prostor a od prostředí získávaly stejné odměny.

Pro uskutečnění těchto experimentů byly vytvořeny skripty v~jazyce Python.
Jeden pro sběr dat a druhý pro jejich následné vyhodnocení a zpracování do grafů a textové podoby.

\section{Experiment 1: Pozorování vývoje chování během tréninku}
\label{sec:experiment-1}

Experiment trénování spočívá v~tom ukázat, jak se agenti postupem trénování zlepšují.
U~agentů DQN a PPO je očekáván nárůst průměrné odměny a snížení průměrné vzdálenosti policistů od Pana~X\@.
Experiment v~průběhu tréninku periodicky provádí simulace her mezi agenty a zaznamenává jejich průběh.

\bigskip
\noindent Hlavními sledovanými parametry jsou:\vspace{-0.5em}
\begin{itemize}
  \item Průměrná odměna Pana~X\vspace{-0.5em}
  \item Průměrná odměna policistů\vspace{-0.5em}
  \item Průměrná vzdálenost policistů od Pana~X\vspace{-0.5em}
  \item Počet vyhraných her Pana~X\vspace{-0.5em}
  \item Počet vyhraných her policistů
\end{itemize}
Experiment sleduje vývoj těchto hodnot po \textbf{1000 iterací}.
Což znamená, že na konci experimentu, podstoupili všichni agenti 1000 iterací tréninku

Simulační hry byly prováděny ve všech možných kombinací algoritmů.
Zkoušeny tedy byly všechny algoritmy jak v~roli Pana~X, tak v~roli policistů.
Simulace se spouštěly po různém počtu trénovacích iterací v~závislosti na aktuálním počtu již provedených iterací.\pagebreak

Počet simulací byl následovný:\vspace{-0.5em}
\begin{itemize}
  \item 50 simulací každých 10 iterací, do 100 trénovacích iterací\vspace{-0.5em}
  \item 50 simulací každých 20 iterací, od 100 do 500 trénovacích iterací\vspace{-0.5em}
  \item 50 simulací každých 50 iterací, od 500 do 1000 trénovacích iterací\vspace{-0.5em}
\end{itemize}

Při učení je nejzajímavější sledovat vývoj chování agentů na začátku trénování.
Proto byly simulace prováděny s~větší četností právě na začátku a postupem trénování byla četnost snižována pro snížení výpočetní a časové zátěže.
Tím bylo zajištěno, že experiment správně a efektivně vyobrazuje jak rychle a jestli, agenti byly schopni pochopit cíl hry.
Sběr dat pro tento experiment trval přibližně 18 hodin.
Při četnějším sběru dat by rapidně narostla časová náročnost experimentu a trval by několik dní.

\subsection{Výsledky experimentu}
\label{subsec:vysledky-experimentu-1}

Při sledování výsledků tohoto experimentu je důležité si uvědomit že díky základní premise hry Scotland Yard je pro policisty vyhrát mnohem složitější než pro Pana~X\@.
A~to z~toho důvodu, že  policisté dokáží vyhrát pouze pokud chytí Pana~X, zato Pan~X může vyhrát jenom na základě uplynutí maximálního počtu kol.

Všechny následující grafy vyobrazují průměrnou odměnu Pana~X a policistů, průměrnou vzdálenost policistů od Pana~X a to vše v~závislosti na počtu trénovacích iterací.

\subsubsection*{Srovnání s~náhodným agentem}
\begin{figure}[H]
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{obrazky-figures/graphs/cop_PPO_mrx_RANDOM}
    \caption{Graf simulace her mezi policisty trénovanými pomocí PPO a Panem X volícím náhodné akce}
    \label{fig:cop_ppo_mrx_random}
  \end{minipage}\hfill
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{obrazky-figures/graphs/cop_RANDOM_mrx_PPO}
      \caption{Graf simulace her mezi Panem X trénovaným pomocí PPO a policisty volícími náhodné akce}
      \label{fig:cop_random_mrx_ppo}
  \end{minipage}
  \end{figure}

Jak jde vidět na grafu~\ref{fig:cop_ppo_mrx_random}, policisté jsou velmi úspěšní ve své strategii.
Vzdálenost mezi policisty, trénovanými pomocí PPO a Panem X, který volí náhodné akce, je po celou dobu velmi malá.
Odměna kterou policisté získávají je velmi vysoká.
Odměna Pana~X je naopak velmi nízká.

Na grafu lze také sledovat že průměrná vzdálenost policistů od Pana~X na začátku trénování rapidně klesá, stejně tak jako klesá odměna Pana~X tím, že se zlepšuje strategie policistů.
Po tomto rapidním poklesu se vzdálenost policistů od Pana~X stabilizuje a odměna Pana~X se drží na nízké hodnotě.

Naopak z~grafu~\ref{fig:cop_random_mrx_ppo} lze pozorovat, že je odměna Pana~X kladná, a až na odchylky vyšší než odměna policistů, kteří se chovají náhodně.
Odměna policistů se pohybuje kolem nuly a odměna Pana~X je stále kladná.
Také vzdálenost policistů od Pana~x je skoro dvojnásobná než v~předchozím případě.

\begin{figure}[H]
  \centering
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{obrazky-figures/graphs/cop_RANDOM_mrx_RANDOM}
    \caption{Graf simulace her mezi policisty a Panem X, kde oba volí náhodné akce}
    \label{fig:cop_random_mrx_random}
  \end{minipage}\hfill
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{obrazky-figures/graphs/cop_PPO_mrx_PPO}
      \caption{Graf simulace her, kde oba agenti volí akce dle modelu PPO}
      \label{fig:cop_ppo_mrx_ppo}
  \end{minipage}
  \caption{Srovnání PPO agenty s~agenty volící náhodné akce}
  \label{fig:srovnani_random_ppo}
  \end{figure}

Na srovnání~\ref{fig:srovnani_random_ppo} lze vidět, že na jak PPO, tak náhodný algoritmus udržují velice stabilní vzdálenost.
U~náhodného chování je to kvůli základní pravděpodobnosti, která musí být průměrně stejná.
U~grafu agentů PPO, to avšak vypovídá o~tom, že policisté dokáží stabilně odhadovat pozici Pana~X a přinejmenším se pohybovat v~jeho blízkém okolí.
Z~grafu\ref{fig:cop_ppo_mrx_ppo} lze pozorovat, že odměna policistů a Pana~X kolísá.
Frekvence tohoto kolísání se postupně jemně zmenšuje.
Z~toho odvozuji, že toto kolísání vypovídá o~tom, že se agenti postupně učí strategii protivníka a snaží se podle ní upravit svoji strategii, aby byli opět lepší.
Což značí, že se obě strategie pomalu přibližují k~optimu.
Předpokládám, že dalším trénováním, by se tento trend potvrdil.
Avšak k~tomu by bylo potřeba zapotřebí větší výpočetní výkon a spoustu času.
Ale jak lze vidět z~výsledků experimentu, i přes relativně malý počet trénovacích iterací se objevuje očekávané chování a rozdíl s~náhodným chováním je markantní.

\subsubsection*{Závěr experimentu pro algoritmus PPO}

Každá možná kombinace využití algoritmů pro policisty a Pana~X byla během experimentu testována celkově 2050$\times$.
Z~tohoto testování vzešly kromě grafů vývoje chování, také údaje zaznamenávající výsledky jednotlivých her.
Tyto výsledky byly zpracovány do tabulek, které zobrazují procentuální počet výher jednotlivých stran v~závislosti na použitém algoritmu.

\begin{figure}[H]
  \centering
  \begin{minipage}{.48\textwidth}
    \begin{table}[H]
    \centering
  \begin{tabular}{@{} c|c|c|c @{}}
  \headercell{\\Policisté} & \multicolumn{3}{c@{}}{Pan~X}\\
  \cmidrule(l){2-4}
  & \multicolumn{1}{c}{PPO}&\multicolumn{1}{c}{Random}&
  \multicolumn{1}{c}{DQN} \\ 
  \hline
  PPO                                    & 38 \% & 67 \% & 65 \% \\ \hline
  Random                                    & 11 \% & 20 \% & 18 \% \\ \hline
  DQN                                    & 10 \% & 17 \% & 19 \% \\ \hline
\end{tabular}
\caption{Zobrazuje procentuální počet výher policistů proti Pánovi~X s~vybraným algoritmem během experimentu}
\label{tab:train_experiment_police_wins}    
\end{table}  
\end{minipage}\hfill
  \begin{minipage}{.48\textwidth}
    \begin{table}[H]
    \centering
    \begin{tabular}{@{} c|c|c|c @{}}
      \headercell{\\Pan~X} & \multicolumn{3}{c@{}}{Policisté}\\
      \cmidrule(l){2-4}
      & \multicolumn{1}{c}{PPO}&\multicolumn{1}{c}{Random}&
      \multicolumn{1}{c}{DQN} \\
      \hline
      PPO                                    & 62 \% & 89 \% & 90 \% \\ \hline
      Random                                    & 33 \% & 80 \% & 83 \% \\ \hline
      DQN                                    & 35 \% & 82 \% & 81 \% \\ \hline
      \end{tabular}
      \caption{Zobrazuje procentuální počet výher Pana~X proti policistům s~vybraným algoritmem během experimentu}
      \label{tab:train_experiment_mrx_wins2}
    \end{table}
  \end{minipage}
  \end{figure}

Pokud zde opět porovnáme výsledky, kdy proti sobě hráli dva náhodní agenti a agenti trénovaní pomocí PPO, lze vidět, že agenti trénovaní pomocí PPO mají výrazně lepší výsledky.
\subsubsection{Ostatní výsledky}

\begin{figure}[H]
  \centering
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{obrazky-figures/graphs/cop_DQN_mrx_RANDOM}
    \caption{Graf simulace hry mezi policisty trénovanými pomocí DQN a Panem X volícím náhodné akce}
    \label{fig:cop_dqn_mrx_random}
  \end{minipage}\hfill
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{obrazky-figures/graphs/cop_RANDOM_mrx_DQN}
      \caption{Graf simulace hry mezi Panem X trénovaným pomocí DQN a policisty volícími náhodné akce}
      \label{fig:cop_random_mrx_dqn}
  \end{minipage}
  \label{fig:srovnani_dqn}
  \end{figure}

Jak již jsem zmiňoval, výsledky trénování agentů pomocí algoritmu DQN bohužel nemají dobré výsledky.
Výsledné chování se velmi podobá náhodnému agentovi.
Z~předchozích prací je známo, že by algoritmus DQN měl mít znatelně lepší výsledky než náhodná metoda~\cite{Hrklova2023thesis}.
Algoritmus DQN je velice citlivý na hyperparametry a je možné, že bylo zvoleno nevhodné nastavení.
Potřebuje také mnohem více trénovacích iterací než PPO\@.
Je tedy možné, že 1000 iterací není dostatečné pro naučení dobré strategie pro algoritmus DQN\@.
Jelikož jsou ale trénovací iterace PPO časově náročnější než iterace DQN, nebylo možné sladit jejich počet.

\emph{Trénování z~tohoto experimentu poslouží jako základ pro další experiment, kde budou simulovány hry s~již natrénovanými agenty.}

\section{Experiment 2: Simulace hry s~již natrénovanými agenty}
\label{sec:experiment-2}

Tento experiment byl proveden za účelem ověření, zda jsou agenti trénováni pomocí algoritmu PPO schopni pochopit cíl hry a naučit se optimální strategii.
Porovnává, jestli již natránovaní agenti z~experimentu 1, trénování tímto způsobem, výrazně liší od agenta který volí pouze náhodné akce a tím prokázat že se chová způsobem, který vede k~vítězství.

Pro ověření zkůtečného výkonu agentů byly provedeny simulace hry mezi již natrénovanými agenty.
Ačkoli již předchozí experiment dokazuje, že agenti trénovaní pomocí algoritmu PPO jsou schopni pochopit cíl hry, je důležité získat skutečná data o~výkonu jednotlivých metod, bez zkreslených výsledků kvůli trénování.

Následující tabulky byly získány ze 10000 simulací pro každou kombinaci již natrénovaných algoritmů.

\begin{figure}[H]
  \centering
  \begin{minipage}{.48\textwidth}
    \begin{table}[H]
    \centering
  \begin{tabular}{@{} c|c|c|c @{}}
  \headercell{\\Policisté} & \multicolumn{3}{c@{}}{Pan~X}\\
  \cmidrule(l){2-4}
  & \multicolumn{1}{c}{PPO}&\multicolumn{1}{c}{Random}&
  \multicolumn{1}{c}{DQN} \\ 
  \hline
  PPO                                    & 26 \% & 72 \% & 64 \% \\ \hline
  Random                                    & 10 \% & 19 \% & 18 \% \\ \hline
  DQN                                    & 8 \% & 17 \% & 17 \% \\ \hline
\end{tabular}
\caption{Zobrazuje procentuální počet výher policistů proti Pánovi~X s~vybraným algoritmem}
\label{tab:simulation_experiment_police_wins}    
\end{table}  
\end{minipage}\hfill
  \begin{minipage}{.48\textwidth}
    \begin{table}[H]
    \centering
    \begin{tabular}{@{} c|c|c|c @{}}
      \headercell{\\Pan~X} & \multicolumn{3}{c@{}}{Policisté}\\
      \cmidrule(l){2-4}
      & \multicolumn{1}{c}{PPO}&\multicolumn{1}{c}{Random}&
      \multicolumn{1}{c}{DQN} \\
      \hline
      PPO                                    & 74 \% & 90 \% & 92 \% \\ \hline
      Random                                    & 28 \% & 81 \% & 82 \% \\ \hline
      DQN                                    & 36 \% & 82 \% & 83 \% \\ \hline
      \end{tabular}
      \caption{Zobrazuje procentuální počet výher Pana~X proti policistům s~vybraným algoritmem}
      \label{tab:train_experiment_mrx_wins}
    \end{table}
  \end{minipage}
  \end{figure}

Z~tabulky lze vyčíst, že policisté trénováni pomocí PPO vyhráli proti Pánovi~x s~náhodným chováním 72 \% her.
V~porovnání s~policisty volící náhodné akce je to výrazně lepší výsledek.
Ti proti Pánovi~x s~náhodným chováním vyhráli pouze 19 \% her.

Z~her vzešly také tyto informace:
\begin{itemize}
  \item Průměrná vzdálenost mezi Panem X a policisty (oba PPO): 4.779692656488397
  \item Průměrná vzdálenost mezi Panem X a policisty (oba náhodné chování): 7.631191103120987
  \item Průměrná vzdálenost mezi Panem X a policisty (oba DQN): 7.869671633109524
\end{itemize}

Z~tohoto experimentu lze tedy vyvodit, že agenti trénovaní pomocí algoritmu PPO prokázali, že se byli schopni naučit optimální strategii i ve hře s~neúplnou informací.

\chapter{Závěr}
\label{ch:zaver}

Provedené experimenty ukázaly, že algoritmus PPO je skutečně vhodný na hry s~neúplnou informací.
Jelikož policisté trénování pomocí PPO dokázali odhadovat přibližnou pozici Pana~X a měli mnohem větší úspěšnost než náhodní agenti.
Agenti se učili formou hraní proti sobě a dokázali pochopit koncept a cíl hry Scotland Yard.
Naučili se v~ní chovat optimálně, přestože začínali s~nulovým věděním o~prostředí.
Bylo příjemným překvapením, že již po pouhých 10 iterací trénování byli agenti schopni projevovat náznaky pochopení cíle hry.

\section*{Možná vylepšení}
\label{sec:mozna-vylepseni}

Bohužel se během experimentu nepovedlo separovat výslednou strategii od frameworku \texttt{Ray.Rlib} a extrahovat ji do podoby kde by již \texttt{Rlib} nebyl potřeba pro její využití.
Při spouštění hry se tedy načítá i celý framework \texttt{Ray.Rlib}, toto velmi zpomaluje načítání hry kdy prvotní načtení trvá několik sekund.
Toto a spoustu dalších potíží jsou důvody proč zvolení knihovny Ray.Rllib zpětně lituji.
Měl jsem spíše algoritmus PPO implementovat svépomocí s~využitím \texttt{Pytorch} či \texttt{Tensorflow}.
Nebo využít knihovnu \texttt{CleanRl} a učení více strategií naráz vyřešit jiným způsobem.

Původním plánem bylo také realizovat možnost hraní hry mezi lidmy a agenty.
Tuto implementaci jsem, avšak přes problémy s~experimenty a algoritmem DQN nestihl dokončit.
Mám za to, že by tato možnost nadále prohloubila výsledky této studie o~zajímavé experimenty, kde by bylo možné sledovat jak si agent PPO vede proti lidskému protivníkovi.

Agenti trénování pomocí algoritmu DQN měli velmi špatné výsledky.
Je vyloučeno, že by byla chyba v~prostředí, udělování odměn či v~pozorování jelikož se agenti trénovaní pomocí PPO učili za stejných podmínek a jejich výsledky byly dobré.
Je možné, že i přes hledání optimálních parametrů s~pomocí \texttt{Ray.Tune} byla zvolena nevhodná konfigurace.
Byť jsem s~těmito parametry dlouhosáhle experimentoval, nebylo dosaženo uspokojivých výsledků.
Navrhuji změnu stavového a akčního prostoru, aby byl kompatibilní s~verzemi studentů z~minulých let, kteří realizovali bakalářskou práci na podobné téma.
A~následně využít jejich implementaci pro další experimenty~\cite{Hrklova2023thesis}.

Dále je zde možnost rozšířit hratelnost hry o~jednotlivé druhy dopravy a k~nim příslušící jízdenky.
Tím by výrazně vzrostla složitost hry a stavový prostor.
Jak ale ukázala práce OpenAI~\cite{Dota2}, algoritmus PPO nemá problém s~obrovským stavovým i akčním prostorem a naučit se i komplikované hry.



%===============================================================================

% Pro kompilaci po částech (viz projekt.tex) nutno odkomentovat
%\end{document}
